{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67478619-abfa-4406-9e29-85e919cca16e",
   "metadata": {},
   "source": [
    "Q1. What is an ensemble technique in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a569e7-3d9a-4781-9daf-14c6f4d4dedc",
   "metadata": {},
   "source": [
    "Ensemble techniques in machine learning involve combining the predictions of multiple machine learning models to improve overall predictive performance. The basic idea behind ensemble methods is that by aggregating the predictions of several models, you can often achieve better results than with a single model. Ensemble methods are commonly used to reduce overfitting, increase robustness, and improve the accuracy and generalization of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b7af63-135d-42e3-8e49-f200c7c829f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a6a132-dc4c-4441-b6f8-c6941d5a1822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "780efc16-4bc2-4a45-a630-82e332fbc5d7",
   "metadata": {},
   "source": [
    "Q2. Why are ensemble techniques used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c40f784-4803-4b56-bbc4-e3b5b9b0b0a0",
   "metadata": {},
   "source": [
    "ensemble techniques are a powerful and widely-used approach in machine learning because they address many common challenges in model development, including accuracy, robustness, and generalization, and they consistently yield competitive results in a wide range of applications.\n",
    "\n",
    "Improved Accuracy: One of the primary motivations for using ensemble techniques is to improve the overall predictive accuracy of a model. By combining the predictions of multiple models, ensembles can often achieve better results than any individual model. This is especially beneficial when dealing with complex, noisy, or high-dimensional data.\n",
    "\n",
    "Reduced Overfitting: Ensembles can help mitigate overfitting, which occurs when a model learns to fit the training data too closely, capturing noise rather than the underlying patterns. By combining multiple models, each with potentially different sources of error, ensembles tend to generalize better to new, unseen data.\n",
    "\n",
    "Robustness: Ensembles are more robust to outliers and anomalies in the data. If an outlier affects the predictions of one model, it may not significantly impact the ensemble's overall prediction, as the errors can cancel each other out.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3891dd-3cee-4510-9097-a94591aca9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014a85af-6b91-4e72-a415-ac493333fc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005f01fb-d53d-4cbd-ace9-94c61f1b00ce",
   "metadata": {},
   "source": [
    "Q3. What is bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a80725f-f964-4277-89cb-0e14db36a5af",
   "metadata": {},
   "source": [
    "\n",
    "Bagging, short for \"Bootstrap Aggregating,\" is an ensemble machine learning technique used to improve the accuracy and robustness of predictive models, particularly decision trees and other high-variance models. Bagging works by training multiple instances of the same base model on different subsets of the training data and then combining their predictions.\n",
    "One of the most well-known algorithms that uses bagging is the Random Forest algorithm, which is an ensemble of decision trees. Random Forest combines bagging with feature randomness, further enhancing its predictive power and reducing the risk of overfitting. Bagging is a fundamental technique in ensemble learning and is widely used in machine learning for improving model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5076d9d4-ffd8-43a2-8dfd-b6b347393368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a51589-e27b-4b89-9deb-907f68924334",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd084e8d-3770-4497-a0b3-a03cb83d7910",
   "metadata": {},
   "source": [
    "Q4. What is boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300658b8-6dd8-4a96-b710-8d9005f28730",
   "metadata": {},
   "source": [
    "Boosting is an ensemble machine learning technique used to improve the accuracy of weak learners (models that are slightly better than random guessing) by combining their predictions in a sequential manner. Unlike bagging, which trains multiple models independently, boosting trains a sequence of models, where each new model is trained to correct the errors made by the previous ones. The primary goal of boosting is to create a strong learner that performs well on the given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba5d9b7-ed7d-4ee3-8301-75b488880411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a89869-b158-4f7f-85be-136b991ce656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ce65d6c-020d-4f2b-b15c-f50cce0b7968",
   "metadata": {},
   "source": [
    "Q5. What are the benefits of using ensemble techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5627d07-3cb7-4da6-b9ed-09673fabd738",
   "metadata": {},
   "source": [
    "Ensemble techniques offer several benefits in machine learning, making them valuable tools for improving model performance and addressing various challenges in predictive modeling. Here are the key benefits of using ensemble techniques:\n",
    "\n",
    "Improved Accuracy: Ensembles often yield higher predictive accuracy compared to individual models. By combining the predictions of multiple models, ensembles can capture a wider range of patterns and reduce errors, leading to more accurate predictions.\n",
    "\n",
    "Reduced Overfitting: Ensemble methods are effective at reducing overfitting, which occurs when a model learns to fit the training data too closely, resulting in poor generalization to new, unseen data. The combination of multiple models with different sources of error helps to smooth out predictions and enhance generalization.\n",
    "\n",
    "Robustness: Ensembles are more robust to noise and outliers in the data. Outliers that significantly impact one model may have less influence on the ensemble's final prediction, as the errors from different models can offset each other.\n",
    "\n",
    "Model Diversity: Ensembles benefit from using diverse base models, which can be trained using different algorithms, subsets of data, or feature representations. Diversity among the models ensures that they make different types of errors, and this diversity can be harnessed to improve overall performance.\n",
    "\n",
    "Handling Bias: Ensembles can reduce bias by combining models with different biases. For instance, one model may have a tendency to under-predict, while another may over-predict. By aggregating their predictions, the ensemble can produce a more balanced and less biased result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95cb73c-bdcd-4ae2-a79f-3104ec038e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfca1fe-d4c8-450e-b7d9-d80654df578c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d70c00-8c67-4302-972c-ec2b6866a1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "946a8999-0fba-4708-be99-0a48e7725b76",
   "metadata": {},
   "source": [
    "Q6. Are ensemble techniques always better than individual models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c57c8d-01b9-49d7-8b78-7cdc0f6ea80d",
   "metadata": {},
   "source": [
    "Ensemble techniques are powerful tools in machine learning and often outperform individual models in terms of predictive accuracy and robustness. However, whether ensemble techniques are always better than individual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3252319-7f0d-413e-ba7d-af75c07b92c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916bc40d-7271-43ad-baa4-ac6b4f2f0095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e57941b1-3b38-45a1-a389-0abb91d1c271",
   "metadata": {},
   "source": [
    "Q7. How is the confidence interval calculated using bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1d3ece-4d06-400b-a3c7-aa421f5def37",
   "metadata": {},
   "source": [
    "Collect your dataset.\n",
    "Create thousands of resamples (with replacement) from the dataset.\n",
    "Calculate the mean for each resample.\n",
    "Calculate the 2.5th percentile and 97.5th percentile of the distribution of resample means for a 95% confidence interval.\n",
    "The resulting interval will give you an estimate of the range within which the true population mean is likely to fall with 95% confidence. The key to the bootstrap method's success is its ability to empirically estimate the distribution of a statistic by resampling from the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b57183e-e80d-437e-b58f-29d9c5967371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfbb92c-b900-4dfc-bb94-e15717ddbbd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da1ee0ee-7ece-4701-8e6e-78be12918a06",
   "metadata": {},
   "source": [
    "Q8. How does bootstrap work and What are the steps involved in bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a06a87-cb9f-4c38-bc7f-10afabe9ca04",
   "metadata": {},
   "source": [
    "Original Dataset (Sample): Start with your original dataset, which contains your observed data or measurements.\n",
    "\n",
    "Resampling (With Replacement): The central concept of bootstrap is to generate a large number of resamples (often thousands or more) by randomly selecting data points from your original dataset with replacement. Each resample has the same size as the original dataset but is composed of data points that may be duplicated.\n",
    "\n",
    "Pseudo-Populations: Each resample represents a pseudo-population. Since resampling is done with replacement, some data points may appear multiple times in a resample, while others may be omitted.\n",
    "\n",
    "Statistical Calculation: For each resample, calculate the statistic or parameter of interest. Common statistics include the mean, median, variance, standard deviation, confidence intervals, and more. These calculations will result in a distribution of statistics.\n",
    "\n",
    "Statistical Analysis: You can perform various statistical analyses on the distribution of statistics obtained from the resamples. Some common applications of bootstrap include:\n",
    "\n",
    "Confidence Intervals: Determine the range within which the true population parameter is likely to fall with a specified level of confidence. This is typically done by calculating percentiles of the distribution.\n",
    "Hypothesis Testing: Conduct hypothesis tests by comparing the observed statistic to the distribution of statistics from the resamples to assess whether a parameter is significantly different from a null hypothesis.\n",
    "Bias Correction: Bootstrap can be used to estimate and correct for bias in parameter estimates.\n",
    "Model Assessment: Evaluate the performance of machine learning models by using bootstrap to estimate prediction error, model selection, and variable importance.\n",
    "Repeat: Steps 2 through 5 are typically repeated thousands of times to generate a robust distribution of the statistic of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff21fc18-e40f-4c41-a837-ed7503a5d1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057398e4-7f84-4978-a670-81d07dd2760d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8549cb-9d40-4ddf-aded-461c64589af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36d19533-d74a-4ee7-ae2b-810f7d25b245",
   "metadata": {},
   "source": [
    "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "bootstrap to estimate the 95% confidence interval for the population mean height.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882cb66f-49cb-45fa-8786-12ed710af474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval for Mean Height: (5.78, 8.20) meters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sample_mean = 15\n",
    "sample_std = 2\n",
    "sample_size = 50\n",
    "\n",
    "# number of bootstrap resamples\n",
    "num_resamples = 10000\n",
    "\n",
    "# create an array to store the bootstrap sample means\n",
    "bootstrap_means = np.zeros(num_resamples)\n",
    "\n",
    "# perform bootstrap resampling\n",
    "for i in range(num_resamples):\n",
    "    bootstrap_sample = np.random.choice(sample_mean, size=sample_size, replace=True)\n",
    "    \n",
    "    bootstrap_means[i] = np.mean(bootstrap_sample)\n",
    "\n",
    "lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "print(f\"95% Confidence Interval for Mean Height: ({lower_bound:.2f}, {upper_bound:.2f}) meters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50d6686a-6e25-468c-aa30-2944a201311c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.94"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0ad391-f3d6-4263-ac10-c88b59c552d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
